{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa11e1d",
   "metadata": {},
   "source": [
    "# Task 1 – Import required libraries\n",
    "For this task you will gather the required libraries to include in your application to write the code.\n",
    "Here, we are importing the following required libraries: csv for writing data to a CSV file, datetime for\n",
    "getting the current date, requests for sending HTTP requests to the website, BeautifulSoup for parsing\n",
    "the HTML source code of the webpage, and time for introducing a delay in our program.\n",
    "This is usually done at the top of the file, for example:\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e62f196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8919b0",
   "metadata": {},
   "source": [
    "# Task 2 – Generating a URL with a function.\n",
    "Now that you have the required imports, you need to define a function that takes in two parameters:\n",
    "position and location. It is useful to define a function to allow for the generation of required data, in this\n",
    "case to form a URL for the web scrape request. If the parameters change, you can change the one\n",
    "function rather than changing the code in multiple places.\n",
    "The parameters in this function are needed to generate the URL of the webpage we want to scrape.\n",
    "Using a function allows you to use a template URL and replace the placeholders with the actual\n",
    "parameter values of position and location. For example, the URL may also include some additional\n",
    "parameters such as locT=C and locId=1139970 that specify the location of the job posting. You can\n",
    "customize these parameters based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "871bd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url(position, location):\n",
    "\n",
    "    # Base URL template\n",
    "    base_url = \"https://realpython.github.io/fake-jobs/\"\n",
    "\n",
    "    # Replace placeholders in the base URL with actual parameter values\n",
    "    formatted_url = base_url.format(position, location)\n",
    "\n",
    "    # Add additional parameters if needed\n",
    "    #formatted_url += \"&locT=C&locId=1139970\"\n",
    "\n",
    "    return formatted_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d338a79",
   "metadata": {},
   "source": [
    "# Task 3 – Extract the Job Data from a single job posting card.\n",
    "The next step is to define a function that will take a single job posting record as input and extract the\n",
    "relevant data from it. The job posting is a Beautiful soup object. This function will be called from within\n",
    "the main() function, which you will define in the next step.\n",
    "To do this, we'll use the BeautifulSoup object to parse the HTML of the individual job posting and extract\n",
    "the desired data using a series of try/except blocks to protect the program and the provide the data with\n",
    "known values in case some data is missing from the posting. \n",
    "For example:\n",
    " try:\n",
    " job_title = atags[0].text.strip()\n",
    " except IndexError:\n",
    " job_title = \"\"\n",
    "It is important to show that you use functions to break problems down into constituent parts to make\n",
    "your program more efficient and maintainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7389528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_data(job_posting):\n",
    "    try:\n",
    "        job_title = job_posting.find(\"h2\", class_=\"title is-5\").text.strip()\n",
    "    except AttributeError:\n",
    "        job_title = \"\"\n",
    "\n",
    "    try:\n",
    "        company = job_posting.find(\"h3\", class_=\"subtitle is-6 company\").text.strip()\n",
    "    except AttributeError:\n",
    "        company = \"\"\n",
    "\n",
    "    try:\n",
    "        location = job_posting.find(\"p\", class_=\"location\").text.strip()\n",
    "    except AttributeError:\n",
    "        location = \"\"\n",
    "\n",
    "    job_data = {\n",
    "        \"Job Title\": job_title,\n",
    "        \"Company\": company,\n",
    "        \"Location\": location,\n",
    "    }\n",
    "\n",
    "    return job_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba553e59",
   "metadata": {},
   "source": [
    "# Task 4 – Define the main function.\n",
    "Every program needs a starting point, and a Python application is no different. Define the main function\n",
    "that takes two parameters: job position and location. This function performs the following steps:\n",
    "1. Set the headers for the HTTP request. A website may block requests from bots, so it's a good\n",
    "idea to set a user agent string.\n",
    "2. Construct the URL for the job search based on the job position and location using the function\n",
    "you created earlier.\n",
    "3. Send an HTTP request to the URL and retrieve the HTML code of the search results page.\n",
    "4. Parse the HTML code using BeautifulSoup and select the HTML elements that contain the job\n",
    "postings (hint: use the Beautiful Soup’s findall method).\n",
    "5. For each posting, extract the job posting information using the helper function from task 3 and\n",
    "store it in a list.\n",
    "6. Write the job posting information to a CSV file.\n",
    "7. Print a success message.\n",
    "Run the program by calling the main function with the required parameters. For example in Jupyter\n",
    "Notebook: main('developer', 'texas')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcd9a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job postings have been successfully scraped and saved to 'jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "def main(position, location):\n",
    "    # Step 1: Set headers for HTTP request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    # Step 2: Construct URL\n",
    "    url = generate_url(position, location)\n",
    "    \n",
    "    # Step 3: Send HTTP request and retrieve HTML\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(\"Failed to retrieve HTML content. Status code:\", response.status_code)\n",
    "        return\n",
    "    \n",
    "    # Step 4: Parse HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Step 5: Extract job posting information\n",
    "    job_postings = soup.find_all(\"div\", class_=\"card-content\")\n",
    "    #print(job_postings)\n",
    "    job_data_list = []\n",
    "    for job_posting in job_postings:\n",
    "        job_data = extract_job_data(job_posting)\n",
    "        job_data_list.append(job_data)\n",
    "    #print(job_data_list)\n",
    "    \n",
    "    \n",
    "    # Step 6: Write job posting information to CSV file\n",
    "    filename = f\"jobs.csv\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        #fieldnames = [\"job_title\", \"company\", \"location\"]  # Add more fields as needed\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"Job Title\", \"Company\", \"Location\"])\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for job_data in job_data_list:\n",
    "            writer.writerow(job_data)\n",
    "    \n",
    "    \n",
    "    # Step 7: Print success message\n",
    "    print(f\"Job postings have been successfully scraped and saved to '{filename}'.\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "main('developer', 'AA')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
